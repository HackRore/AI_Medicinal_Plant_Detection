{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "08412be3",
   "metadata": {},
   "source": [
    "# Medicinal Plant Detection AI — Implementation Notebook\n",
    "\n",
    "This notebook implements the **Phase 1–5** pipeline using the **Indian Medicinal Leaves** dataset from Kaggle. It is organized to be a practical demo for your project guide.\n",
    "\n",
    "**Before running:**\n",
    "1. Download the dataset from Kaggle (https://www.kaggle.com/datasets/aryashah2k/indian-medicinal-leaves-dataset) and\n",
    "   extract it into `./dataset/` such that images are inside subfolders per class: `dataset/<class_name>/*.jpg`.\n",
    "2. Use a Python 3.8+ environment. Recommended: create a virtualenv and install dependencies listed in the first cell.\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "9230b0e0",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Install required packages (uncomment and run if needed)\n",
    "!pip install -q tensorflow==2.12.0 opencv-python-headless matplotlib scikit-learn pandas streamlit nbconvert\n",
    "# If running on Colab, use: \n",
    "# !pip install -q tensorflow==2.12.0 opencv-python-headless\n",
    "\n",
    "print('Install packages if needed and ensure dataset is placed at ./dataset/')\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "1d313daf",
   "metadata": {},
   "outputs": [],
   "source": [
    "import os\n",
    "import random\n",
    "import numpy as np\n",
    "import pandas as pd\n",
    "import matplotlib.pyplot as plt\n",
    "from pathlib import Path\n",
    "\n",
    "import tensorflow as tf\n",
    "from tensorflow.keras.preprocessing.image import ImageDataGenerator\n",
    "from tensorflow.keras.applications import MobileNetV2\n",
    "from tensorflow.keras.applications.mobilenet_v2 import preprocess_input\n",
    "from tensorflow.keras import layers, models\n",
    "from tensorflow.keras.optimizers import Adam\n",
    "\n",
    "from sklearn.ensemble import RandomForestClassifier\n",
    "from sklearn.metrics import classification_report, confusion_matrix\n",
    "import sqlite3\n",
    "\n",
    "print('TensorFlow version:', tf.__version__)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "5fa96212",
   "metadata": {},
   "source": [
    "## Phase 1 — Dataset Preparation\n",
    "\n",
    "Load dataset directory structure and show class distribution."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "fd948d84",
   "metadata": {},
   "outputs": [],
   "source": [
    "DATA_DIR = Path('dataset')\n",
    "assert DATA_DIR.exists(), 'Please download and extract dataset to ./dataset'\n",
    "\n",
    "# list classes\n",
    "classes = [p.name for p in DATA_DIR.iterdir() if p.is_dir()]\n",
    "print('Found classes:', len(classes))\n",
    "for c in classes[:20]:\n",
    "    print('-', c)\n",
    "\n",
    "# quick count\n",
    "counts = {c: len(list((DATA_DIR/c).glob('*'))) for c in classes}\n",
    "counts_df = pd.DataFrame(list(counts.items()), columns=['class','count']).sort_values('count', ascending=False)\n",
    "counts_df.head()\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "9a3d86da",
   "metadata": {},
   "source": [
    "### Create train/val/test splits and data generators (ImageDataGenerator)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "b6ab63ef",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Create directory structure for train/val/test splits (if not already present)\n",
    "from sklearn.model_selection import train_test_split\n",
    "\n",
    "ROOT = Path('data_split')\n",
    "if not ROOT.exists():\n",
    "    ROOT.mkdir()\n",
    "    for split in ['train','val','test']:\n",
    "        (ROOT/split).mkdir(exist_ok=True)\n",
    "\n",
    "    # split per class\n",
    "    for cls in classes:\n",
    "        imgs = list((DATA_DIR/cls).glob('*'))\n",
    "        imgs = [p for p in imgs if p.suffix.lower() in ['.jpg','.jpeg','.png']]\n",
    "        train, temp = train_test_split(imgs, test_size=0.3, random_state=42)\n",
    "        val, test = train_test_split(temp, test_size=0.5, random_state=42)\n",
    "        for p in train:\n",
    "            (ROOT/'train'/cls).mkdir(parents=True, exist_ok=True)\n",
    "            os.symlink(p.resolve(), (ROOT/'train'/cls/p.name))\n",
    "        for p in val:\n",
    "            (ROOT/'val'/cls).mkdir(parents=True, exist_ok=True)\n",
    "            os.symlink(p.resolve(), (ROOT/'val'/cls/p.name))\n",
    "        for p in test:\n",
    "            (ROOT/'test'/cls).mkdir(parents=True, exist_ok=True)\n",
    "            os.symlink(p.resolve(), (ROOT/'test'/cls/p.name))\n",
    "    print('Data split created at', ROOT)\n",
    "else:\n",
    "    print('Using existing splits at', ROOT)\n",
    "\n",
    "# Create generators\n",
    "IMG_SIZE = (224,224)\n",
    "BATCH = 32\n",
    "train_datagen = ImageDataGenerator(preprocessing_function=preprocess_input,\n",
    "                                   rotation_range=20, horizontal_flip=True, width_shift_range=0.1, height_shift_range=0.1)\n",
    "val_datagen = ImageDataGenerator(preprocessing_function=preprocess_input)\n",
    "\n",
    "train_gen = train_datagen.flow_from_directory(ROOT/'train', target_size=IMG_SIZE, batch_size=BATCH, class_mode='categorical')\n",
    "val_gen = val_datagen.flow_from_directory(ROOT/'val', target_size=IMG_SIZE, batch_size=BATCH, class_mode='categorical')\n",
    "\n",
    "num_classes = len(train_gen.class_indices)\n",
    "print('Number of classes:', num_classes)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "e1254b4c",
   "metadata": {},
   "source": [
    "## Phase 2 — Model Development (Transfer Learning)\n",
    "\n",
    "We build a MobileNetV2 transfer learning model as a baseline."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "b145fb8d",
   "metadata": {},
   "outputs": [],
   "source": [
    "base = MobileNetV2(weights='imagenet', include_top=False, input_shape=(224,224,3), pooling='avg')\n",
    "for layer in base.layers:\n",
    "    layer.trainable = False\n",
    "\n",
    "inputs = layers.Input(shape=(224,224,3))\n",
    "x = base(inputs, training=False)\n",
    "x = layers.Dense(512, activation='relu')(x)\n",
    "x = layers.Dropout(0.3)(x)\n",
    "outputs = layers.Dense(num_classes, activation='softmax')(x)\n",
    "model = models.Model(inputs, outputs)\n",
    "model.compile(optimizer=Adam(1e-4), loss='categorical_crossentropy', metrics=['accuracy'])\n",
    "model.summary()\n",
    "\n",
    "# Train for a few epochs (for demo; increase for real training)\n",
    "EPOCHS = 5\n",
    "history = model.fit(train_gen, validation_data=val_gen, epochs=EPOCHS)\n",
    "model.save('models/mobilenetv2_finetuned.h5')\n",
    "print('Model saved to models/mobilenetv2_finetuned.h5')\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "1aa02077",
   "metadata": {},
   "source": [
    "### Evaluate on test set"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "68183682",
   "metadata": {},
   "outputs": [],
   "source": [
    "test_gen = val_datagen.flow_from_directory(ROOT/'test', target_size=IMG_SIZE, batch_size=BATCH, class_mode='categorical', shuffle=False)\n",
    "\n",
    "preds = model.predict(test_gen, verbose=1)\n",
    "y_true = test_gen.classes\n",
    "y_pred = preds.argmax(axis=1)\n",
    "print(classification_report(y_true, y_pred, target_names=list(test_gen.class_indices.keys())))\n",
    "\n",
    "cm = confusion_matrix(y_true, y_pred)\n",
    "print('Confusion matrix shape:', cm.shape)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "baf51f79",
   "metadata": {},
   "source": [
    "## Phase 3 — Model Optimization (Ensemble with Classical ML)\n",
    "\n",
    "Extract deep features using backbone and train a RandomForest classifier on them as an ensemble component."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "ec3fdf26",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Extract deep features from the backbone for each image in train/val/test\n",
    "from tensorflow.keras.preprocessing import image\n",
    "\n",
    "backbone = tf.keras.Model(inputs=base.input, outputs=base.output)\n",
    "\n",
    "def extract_features_from_generator(gen, backbone_model):\n",
    "    features = []\n",
    "    labels = []\n",
    "    gen.reset()\n",
    "    for i in range(len(gen)):\n",
    "        x, y = gen[i]\n",
    "        feats = backbone_model.predict(x)\n",
    "        features.append(feats)\n",
    "        labels.append(y)\n",
    "    X = np.vstack(features)\n",
    "    y = np.vstack(labels).argmax(axis=1)\n",
    "    return X, y\n",
    "\n",
    "X_train, y_train = extract_features_from_generator(train_gen, backbone)\n",
    "X_val, y_val = extract_features_from_generator(val_gen, backbone)\n",
    "\n",
    "print('Feature shapes:', X_train.shape, X_val.shape)\n",
    "\n",
    "rf = RandomForestClassifier(n_estimators=100, random_state=42)\n",
    "rf.fit(X_train, y_train)\n",
    "print('RF trained. Validation score:', rf.score(X_val, y_val))\n",
    "\n",
    "# Save RF with joblib\n",
    "import joblib\n",
    "os.makedirs('models', exist_ok=True)\n",
    "joblib.dump(rf, 'models/rf_on_deep_features.pkl')\n",
    "print('Saved RF to models/rf_on_deep_features.pkl')\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "19b6f58b",
   "metadata": {},
   "source": [
    "## Phase 4 — Explainable AI (Grad-CAM)\n",
    "\n",
    "Utility to generate Grad-CAM heatmap for Keras models."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "b8fef120",
   "metadata": {},
   "outputs": [],
   "source": [
    "import cv2\n",
    "import tensorflow as tf\n",
    "\n",
    "def make_gradcam_heatmap(img_array, model, last_conv_layer_name):\n",
    "    # img_array: preprocessed image (1,H,W,3)\n",
    "    grad_model = tf.keras.models.Model([model.inputs], [model.get_layer(last_conv_layer_name).output, model.output])\n",
    "    with tf.GradientTape() as tape:\n",
    "        conv_outputs, predictions = grad_model(img_array)\n",
    "        class_idx = tf.argmax(predictions[0])\n",
    "        loss = predictions[:, class_idx]\n",
    "    grads = tape.gradient(loss, conv_outputs)\n",
    "    pooled_grads = tf.reduce_mean(grads, axis=(0,1,2))\n",
    "    conv_outputs = conv_outputs[0]\n",
    "    heatmap = conv_outputs @ pooled_grads[..., tf.newaxis]\n",
    "    heatmap = tf.squeeze(heatmap).numpy()\n",
    "    heatmap = np.maximum(heatmap, 0)\n",
    "    heatmap /= (heatmap.max() + 1e-8)\n",
    "    heatmap = cv2.resize(heatmap, (224,224))\n",
    "    return heatmap\n",
    "\n",
    "# Example on a test image\n",
    "img_path = list((ROOT/'test').rglob('*.*'))[0]\n",
    "img = image.load_img(img_path, target_size=(224,224))\n",
    "img_arr = image.img_to_array(img)\n",
    "img_proc = preprocess_input(img_arr.copy())\n",
    "img_input = np.expand_dims(img_proc, 0)\n",
    "\n",
    "# find last conv layer name\n",
    "last_conv = None\n",
    "for l in reversed(model.layers):\n",
    "    if 'conv' in l.name:\n",
    "        last_conv = l.name\n",
    "        break\n",
    "print('Last conv layer:', last_conv)\n",
    "heatmap = make_gradcam_heatmap(img_input, model, last_conv)\n",
    "\n",
    "# overlay\n",
    "orig = (img_arr / 255.0)\n",
    "heatmap_rgb = cv2.applyColorMap(np.uint8(255*heatmap), cv2.COLORMAP_JET)\n",
    "heatmap_rgb = cv2.cvtColor(heatmap_rgb, cv2.COLOR_BGR2RGB) / 255.0\n",
    "overlay = 0.6*orig + 0.4*heatmap_rgb\n",
    "\n",
    "plt.figure(figsize=(8,4))\n",
    "plt.subplot(1,2,1); plt.title('Original'); plt.axis('off'); plt.imshow(orig)\n",
    "plt.subplot(1,2,2); plt.title('Grad-CAM'); plt.axis('off'); plt.imshow(overlay)\n",
    "plt.show()\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "5ce92563",
   "metadata": {},
   "source": [
    "### Complementary feature — Recommendation (text-sim based)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "c12fec89",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Simple TF-IDF recommendation using sqlite DB created in the project\n",
    "DB = 'data/medicinal_info.db'\n",
    "conn = sqlite3.connect(DB)\n",
    "# if db empty, create demo entries\n",
    "c = conn.cursor()\n",
    "c.execute('CREATE TABLE IF NOT EXISTS plants (species TEXT PRIMARY KEY, medicinal_uses TEXT)')\n",
    "rows = c.execute('SELECT COUNT(*) FROM plants').fetchone()[0]\n",
    "if rows == 0:\n",
    "    demo = [\n",
    "        ('Azadirachta_indica','antibacterial; skin infection; wound healing'),\n",
    "        ('Ocimum_sanctum','respiratory; immunity; anti-inflammatory'),\n",
    "        ('Aloe_vera','wound healing; skin soothing; digestive')\n",
    "    ]\n",
    "    c.executemany('INSERT OR IGNORE INTO plants (species, medicinal_uses) VALUES (?,?)', demo)\n",
    "    conn.commit()\n",
    "\n",
    "species_list = [r[0] for r in c.execute('SELECT species FROM plants').fetchall()]\n",
    "texts = [r[0] for r in c.execute('SELECT medicinal_uses FROM plants').fetchall()]\n",
    "\n",
    "from sklearn.feature_extraction.text import TfidfVectorizer\n",
    "from sklearn.metrics.pairwise import cosine_similarity\n",
    "vec = TfidfVectorizer(stop_words='english')\n",
    "X = vec.fit_transform(texts)\n",
    "\n",
    "query_idx = 0\n",
    "sims = cosine_similarity(X[query_idx], X)[0]\n",
    "ranked = sorted(list(enumerate(sims)), key=lambda x:x[1], reverse=True)\n",
    "print('Recommendations for', species_list[query_idx])\n",
    "for idx,score in ranked[1:3]:\n",
    "    print('-', species_list[idx], f'(score={score:.2f})')\n",
    "\n",
    "conn.close()\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "3c7f8c4f",
   "metadata": {},
   "source": [
    "## Phase 5 — Integration & Testing\n",
    "\n",
    "Below is a minimal Streamlit app snippet you can use to integrate the model for demo."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "eb65489a",
   "metadata": {},
   "outputs": [],
   "source": [
    "streamlit_code = '''\n",
    "import streamlit as st\n",
    "from tensorflow.keras.models import load_model\n",
    "from tensorflow.keras.applications.mobilenet_v2 import preprocess_input\n",
    "import numpy as np\n",
    "import cv2\n",
    "\n",
    "model = load_model('models/mobilenetv2_finetuned.h5')\n",
    "\n",
    "st.title('Medicinal Plant Detector')\n",
    "img_file = st.file_uploader('Upload leaf image', type=['jpg','png','jpeg'])\n",
    "if img_file:\n",
    "    arr = np.frombuffer(img_file.read(), np.uint8)\n",
    "    img = cv2.imdecode(arr, cv2.IMREAD_COLOR)\n",
    "    img = cv2.cvtColor(img, cv2.COLOR_BGR2RGB)\n",
    "    st.image(img, caption='Uploaded', use_column_width=True)\n",
    "    x = cv2.resize(img, (224,224))\n",
    "    x = preprocess_input(x.astype('float32'))\n",
    "    pred = model.predict(np.expand_dims(x,0))[0]\n",
    "    label = pred.argmax()\n",
    "    st.write('Predicted class index:', int(label))\n",
    "'''\n",
    "\n",
    "with open('streamlit_demo.py','w') as f:\n",
    "    f.write(streamlit_code)\n",
    "\n",
    "print('Streamlit demo saved to streamlit_demo.py')\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "6d32bf44",
   "metadata": {},
   "source": [
    "### Notebook Ready\n",
    "\n",
    "You can download the notebook file from the workspace. Run cells step-by-step. For a production run, increase epochs and use GPU runtime."
   ]
  }
 ],
 "metadata": {
  "language_info": {
   "name": "python"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
